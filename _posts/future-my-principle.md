---
    layout: post
    publish:false
    title: My Principle
---

It seems like everyone has seen Bret Victor’s talk [_Inventing on Principle_](http://vimeo.com/36579366) by now. I’ve seen a lot of discussion about whether or not his demos are viable – can they scale to real software, or do they only work on simple examples? I’m sure answers to those questions will start appearing as things like [Light Table](http://www.kickstarter.com/projects/ibdknox/light-table) are developed.

However, I haven’t seen much discussion about the “principle” aspect of Bret’s talk. I’m sure people are thinking about it, but maybe it’s just more personal – you might think about what your principle is, but you don’t have to share that with others. Of course, if you are following the activism aspect, then glimpses into your principle are given through your actions. I’m sure there are plenty of developers who have this pretty well thought out, and follow through on it. I have seen many activist programmers, some of whose causes seem disconnected to me (but likely I just haven’t put enough effort into finding the commonality).

I have definitely thought about my principle, but have thus far hesitated to take much visible action based on it. I have been working on things behind closed doors (well, publicly, but without drawing any attention to them) that achieve my principle, but I have not taken the activism step – thinking, “once this is done, it will be apparent that this is right, and I won’t have to be an activist.”

But here is my principle: **Everyone should be able to harness the full capabilities of computers to answer questions for themselves.** In some ways, this is similar to the idea behind [Code Year](http://codeyear.com/), but while I disagree with [Jeff Atwood’s stance](http://www.codinghorror.com/blog/2012/05/please-dont-learn-to-code.html) (_shouldn’t_ everyone have basic plumbing skills?), I also think Code Year takes the wrong approach. Even most people who make their living from writing code often do little more than type out the same abstractions long-hand over and over – if you can give an estimate for a programming task, then that’s a task that shouldn’t need to be done. Teaching everyone to write Ruby or Python should not be the goal. Everyone should only need to be taught basic principles – logic, some mathematics, etc. – and as little as possible on top of that to convert that into answers to their questions. Is Ruby “as little as possible?” not a chance. The current state of high-level languages is so far below the level necessary to make it possible to state these questions directly. There have been steps taken upward, but they are few and far between. Ruby and Python take none. They convert subsets of existing languages into a syntax that people who are already programmers find familiar (but in that translation, they lose so much) – these are not steps forward, but backward.

So, with my principle, I am attempting to lift languages up as far as they can go. Am I creating my own language? Yes. Is it the language that will achieve that goal? I don't think that there is or will be such a language. I think of my language as the first step toward a new way of building languages. My language is something like an extremely high-level VM and language toolkit. Like Lisp before it, depending on the concepts being discussed, my language might be described as high-level or low-level. The reason for this is that it directly implements a fundamental model of computation, but that model provides primitives that directly offer things we often think of as high-level features ([message-passing parallelism](http://en.wikipedia.org/wiki/Message_passing), distributed programming tools, [capabilities](http://en.wikipedia.org/wiki/Capability-based_security), etc.).

This language can be used directly as a general-purpose programming language that supports lisp-like macros (is there another way to say “just normal macros that, unlike `#define` and `eval`ing a string, don’t suck”), which, as we all know, allow you to turn the language into anything you want. And I think this is why Lisp isn't more popular – why isn’t Lisp used like LLVM or something, with internal DSLs being built to the level where you might not even realize it’s Lisp underneath it all[^1][^2].

I still occasionally hear the claim that if a language is Turing-complete, it should be just as good as any other language. I find it hard to believe that anyone saying that is being sincere. Would you rather code in Ruby (or language of your choice) or machine code? Both are Turing-complete, but one is so much more _expressive_ than the other. Expressivity is actually a well-defined term in computer science. [λCalculus](http://en.wikipedia.org/wiki/Lambda_calculus) and Turing machines are pretty much at the bottom, yet they still dominate the language design. Recently, [the Actor model](http://en.wikipedia.org/wiki/Actor_model) (well, approximations to it[^3]) has been attached to various languages to provide parallelism (and sometimes distributed programming). Near the top are models that directly provide parallel computation and make it possible to encode (i.e. implement) the lambda calculus (with parallelism) in a handful of expressions.

The idea is to have “general-purpose” languages that provide these formalisms directly. These are probably a small set of languages, written as new more (or differently) expressive formalisms are developed, with much of the effort spent on compilers – optimizing, JIT, etc.

One of the side benefits of this approach, is that new formalisms often differ evolutionarily from existing ones – they combine features from disparate calculi or replace one operator with another that is slightly more expressive. This means that much of what was true for the last round of compilers is true for the new one. In my progression to my current language

If LLVM is a footstool toward building a language, these are elevators. In these languages, 

[^1]: Yes, there are tons of internal DSLs built in Lisp, but they are generally meant to be used to simplify some component of the program, not to be used entirely in place of Lisp.
[^2]: Part of the problem is that Lisp’s reader macros and readtables do not provide an ideal parser definition system to do this. Lisp programmers are often told to avoid defining reader macros, because they are not easily controlled or composed (libraries like [named-readtables](http://common-lisp.net/project/named-readtables/) aim to remedy this, with more success than could be expected).